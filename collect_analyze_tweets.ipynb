{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import json\n",
    "from datetime import date\n",
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'credentials_file': 'twitter_credentials.json', 'pysad_path': '../', 'data_path': '../tweetdata/', 'results_path': '../resultsdata/', 'accounts_file': 'initial_accounts.json'}\n"
     ]
    }
   ],
   "source": [
    "# The code require the pysad module\n",
    "# This module has to be installed separately (see the README file)\n",
    "# Once installed, the path where the pysad module is located must be set in the config.json file under 'pysad_path'\n",
    "import twitutils\n",
    "init_data = twitutils.init()\n",
    "print(init_data)\n",
    "import sys\n",
    "sys.path.append(init_data['pysad_path'])\n",
    "\n",
    "import pysad\n",
    "#import pysad.utils\n",
    "import pysad.pysad.collect as pcollect\n",
    "import pysad.pysad.twitter as ptwitter\n",
    "from pysad.pysad.NodeInfo import SynthNodeInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(init_data['credentials_file']) as json_file:\n",
    "    cred = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_handle = ptwitter.TwitterNetwork(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'twitutils' from '/home/benjamin/Documents/EPFL/Research/sad/twittexp/twitutils.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pcollect)\n",
    "importlib.reload(ptwitter)\n",
    "importlib.reload(twitutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_accounts = ptwitter.initial_accounts(init_data['accounts_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benjamin', 'Chloroquine', 'Chloroquine2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_accounts.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "######Choose a category##############    \n",
    "category_name = 'Benjamin'\n",
    "category_name = 'Chloroquine2'\n",
    "username_list = init_accounts.accounts(category_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_handle.rules['min_mentions'] = 1 # minimal number of mentions of a user to be followed\n",
    "graph_handle.rules['max_day_old'] = 5 # number max of days in the past\n",
    "exploration_depth = 3 # mention of mention of mention of ... up to exploration depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Twitter API returned error 404 for user ChabriereEric.\n",
      "WARNING:root:Unauthorized access to user A_fondiste. Skipping.\n",
      "WARNING:root:Unauthorized access to user COVID19. Skipping.\n",
      "WARNING:root:Unauthorized access to user dmascret. Skipping.\n",
      "WARNING:root:Unauthorized access to user CanalBagatelle. Skipping.\n",
      "WARNING:root:Unauthorized access to user DROIT_AU_BUT_TV. Skipping.\n",
      "WARNING:root:Unauthorized access to user MedicusFR. Skipping.\n",
      "WARNING:root:Unauthorized access to user _M_Avocat. Skipping.\n",
      "WARNING:root:Unauthorized access to user AlbaRockWater. Skipping.\n",
      "WARNING:root:Unauthorized access to user GregoireD0909. Skipping.\n",
      "WARNING:root:Unauthorized access to user GucciIg. Skipping.\n",
      "ERROR:root:Twitter API returned error 404 for user Zeteticum.\n",
      "WARNING:root:Unauthorized access to user anniz_doc. Skipping.\n",
      "WARNING:root:Unauthorized access to user OUECHFADA. Skipping.\n",
      "WARNING:root:Unauthorized access to user tepatriote. Skipping.\n"
     ]
    }
   ],
   "source": [
    "total_user_list, total_nodes_df, total_edges_df, node_acc = pcollect.spiky_ball(username_list, \n",
    "                                                                               graph_handle, \n",
    "                                                                               exploration_depth=exploration_depth,\n",
    "                                                                                mode='constant',\n",
    "                                                                               random_subset_size=400,\n",
    "                                                                     node_acc=SynthNodeInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users mentioned: 759\n"
     ]
    }
   ],
   "source": [
    "print('Total number of users mentioned:',len(total_user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date string = 20200903\n",
      "Cleaned path ../tweetdata/Chloroquine2/20200903\n"
     ]
    }
   ],
   "source": [
    "# Save the collected data in json files\n",
    "\n",
    "# create the path to save the experiment indexed with the date of today\n",
    "today = date.today()\n",
    "date_string = today.strftime(\"%Y%m%d\")\n",
    "print(\"date string =\", date_string)\n",
    "\n",
    "tweet_data_path_list = [init_data['data_path'], category_name, date_string]\n",
    "results_data_path_list = [init_data['results_path'], category_name, date_string]\n",
    "\n",
    "#tweet_data_path = ''.join(tweet_data_path_list)\n",
    "#results_data_path = ''.join(results_data_path_list)\n",
    "\n",
    "# Initialize folders (create or clean them if they exist)\n",
    "# Set erase=False if you need to keep the previous collection\n",
    "tweet_data_path = twitutils.initialize_folder(tweet_data_path_list, erase=True)\n",
    "results_data_path = twitutils.initialize_folder(results_data_path_list, erase=False)\n",
    "\n",
    "# save data\n",
    "pcollect.save_data(total_nodes_df,total_edges_df,tweet_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edges_df.reset_index().to_json('test1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgestest = pd.read_json('test1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChtiKiff69</td>\n",
       "      <td>nrenard75</td>\n",
       "      <td>[1300374970602795008, 1300375027934785539, 130...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCIPolitique</td>\n",
       "      <td>LCI</td>\n",
       "      <td>[1300667468558544896, 1300678398612836353, 130...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fakehistoryhunt</td>\n",
       "      <td>kimaikia_</td>\n",
       "      <td>[1301229520238895104, 1301231786886864898, 130...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fakehistoryhunt</td>\n",
       "      <td>coimbraajoaoo</td>\n",
       "      <td>[1301241558348558336, 1301243330387415041, 130...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jcl_lambert</td>\n",
       "      <td>philippefroguel</td>\n",
       "      <td>[1299736675833327618, 1299737313757605893, 129...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45171</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>huguesdelestre</td>\n",
       "      <td>[1300833504343793669]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45172</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>holy_Phoenixx</td>\n",
       "      <td>[1300419135042707458]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45173</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>heyrudy_</td>\n",
       "      <td>[1301234236045881347]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45174</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>halleberry</td>\n",
       "      <td>[1300059686755094529]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45175</th>\n",
       "      <td>Quantic_QBit</td>\n",
       "      <td>StoneYo5</td>\n",
       "      <td>[1301118634459369472]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user         mentions  \\\n",
       "0           ChtiKiff69        nrenard75   \n",
       "1         LCIPolitique              LCI   \n",
       "2      fakehistoryhunt        kimaikia_   \n",
       "3      fakehistoryhunt    coimbraajoaoo   \n",
       "4          jcl_lambert  philippefroguel   \n",
       "...                ...              ...   \n",
       "45171    ThomasinParis   huguesdelestre   \n",
       "45172    ThomasinParis    holy_Phoenixx   \n",
       "45173    ThomasinParis         heyrudy_   \n",
       "45174    ThomasinParis       halleberry   \n",
       "45175     Quantic_QBit         StoneYo5   \n",
       "\n",
       "                                                tweet_id  weight  \n",
       "0      [1300374970602795008, 1300375027934785539, 130...     185  \n",
       "1      [1300667468558544896, 1300678398612836353, 130...     168  \n",
       "2      [1301229520238895104, 1301231786886864898, 130...     151  \n",
       "3      [1301241558348558336, 1301243330387415041, 130...     123  \n",
       "4      [1299736675833327618, 1299737313757605893, 129...      92  \n",
       "...                                                  ...     ...  \n",
       "45171                              [1300833504343793669]       1  \n",
       "45172                              [1300419135042707458]       1  \n",
       "45173                              [1301234236045881347]       1  \n",
       "45174                              [1300059686755094529]       1  \n",
       "45175                              [1301118634459369472]       1  \n",
       "\n",
       "[45176 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>mentions</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChtiKiff69</td>\n",
       "      <td>nrenard75</td>\n",
       "      <td>[1300374970602795008, 1300375027934785539, 130...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LCIPolitique</td>\n",
       "      <td>LCI</td>\n",
       "      <td>[1300667468558544896, 1300678398612836353, 130...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fakehistoryhunt</td>\n",
       "      <td>kimaikia_</td>\n",
       "      <td>[1301229520238895104, 1301231786886864898, 130...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fakehistoryhunt</td>\n",
       "      <td>coimbraajoaoo</td>\n",
       "      <td>[1301241558348558336, 1301243330387415041, 130...</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jcl_lambert</td>\n",
       "      <td>philippefroguel</td>\n",
       "      <td>[1299736675833327618, 1299737313757605893, 129...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45171</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>huguesdelestre</td>\n",
       "      <td>[1300833504343793669]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45172</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>holy_Phoenixx</td>\n",
       "      <td>[1300419135042707458]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45173</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>heyrudy_</td>\n",
       "      <td>[1301234236045881347]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45174</th>\n",
       "      <td>ThomasinParis</td>\n",
       "      <td>halleberry</td>\n",
       "      <td>[1300059686755094529]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45175</th>\n",
       "      <td>Quantic_QBit</td>\n",
       "      <td>StoneYo5</td>\n",
       "      <td>[1301118634459369472]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45176 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user         mentions  \\\n",
       "0           ChtiKiff69        nrenard75   \n",
       "1         LCIPolitique              LCI   \n",
       "2      fakehistoryhunt        kimaikia_   \n",
       "3      fakehistoryhunt    coimbraajoaoo   \n",
       "4          jcl_lambert  philippefroguel   \n",
       "...                ...              ...   \n",
       "45171    ThomasinParis   huguesdelestre   \n",
       "45172    ThomasinParis    holy_Phoenixx   \n",
       "45173    ThomasinParis         heyrudy_   \n",
       "45174    ThomasinParis       halleberry   \n",
       "45175     Quantic_QBit         StoneYo5   \n",
       "\n",
       "                                                tweet_id  weight  \n",
       "0      [1300374970602795008, 1300375027934785539, 130...     185  \n",
       "1      [1300667468558544896, 1300678398612836353, 130...     168  \n",
       "2      [1301229520238895104, 1301231786886864898, 130...     151  \n",
       "3      [1301241558348558336, 1301243330387415041, 130...     123  \n",
       "4      [1299736675833327618, 1299737313757605893, 129...      92  \n",
       "...                                                  ...     ...  \n",
       "45171                              [1300833504343793669]       1  \n",
       "45172                              [1300419135042707458]       1  \n",
       "45173                              [1301234236045881347]       1  \n",
       "45174                              [1300059686755094529]       1  \n",
       "45175                              [1301118634459369472]       1  \n",
       "\n",
       "[45176 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_edges_df.re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved data into an edge table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'twitclusters' from '/home/benjamin/Documents/EPFL/Research/sad/twittexp/twitclusters.py'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysad.pysad.graph as pgraph\n",
    "import twitclusters\n",
    "import importlib\n",
    "importlib.reload(ptwitter)\n",
    "importlib.reload(twitclusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of tweets date from 29/08/2020 to 03/09/2020\n"
     ]
    }
   ],
   "source": [
    "node_df, edge_df = pcollect.load_data(tweet_data_path)\n",
    "end_date = max(node_df['created_at']).strftime(\"%d/%m/%Y\") \n",
    "start_date = min(node_df['created_at']).strftime(\"%d/%m/%Y\")\n",
    "print('Range of tweets date from {} to {}'.format(start_date,end_date))\n",
    "node_df = ptwitter.reshape_node_data(node_df)\n",
    "edge_df = ptwitter.reshape_edge_data(edge_df,min_weight=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.graph)\n",
    "importlib.reload(pysad.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period from 29/08/2020 to 03/09/2020.\n"
     ]
    }
   ],
   "source": [
    "MIN_WEIGHT = 2\n",
    "MIN_DEGREE = 2 # Minimal number of connections in the graph\n",
    "\n",
    "G = pgraph.graph_from_edgeslist(edge_df, MIN_WEIGHT)\n",
    "#G = pysad.graph.graph_from_edgeslist(df_pop,DEGREE_MIN)\n",
    "G = pgraph.add_edges_attributes(G,edge_df)\n",
    "G = pgraph.add_node_attributes(G,node_df)\n",
    "G = pgraph.reduce_graph(G,MIN_DEGREE)\n",
    "G = pgraph.handle_spikyball_neighbors(G,graph_handle)#,remove=False)\n",
    "# Warning, graph properties are not saved by networkx in gexf files except graph name\n",
    "G.graph['end_date'] = end_date \n",
    "G.graph['start_date'] = start_date\n",
    "G.graph['name'] = category_name + ' ' + G.graph['start_date'] + ' - ' + G.graph['end_date'] \n",
    "print('Period from {} to {}.'.format(G.graph['start_date'],G.graph['end_date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection to get the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pysad.pysad.graph' from '../pysad/pysad/graph.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(twitclusters)\n",
    "importlib.reload(pgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "G,clusters = pgraph.detect_communities(G)\n",
    "G.nb_communities = len(clusters)\n",
    "#c_connectivity = pysad.clusters.cluster_connectivity(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = pgraph.remove_small_communities(G,clusters,min_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ../resultsdata/Chloroquine2/20200903/Chloroquine25_t1_md2_graph.gexf\n",
      "Wrote ../resultsdata/Chloroquine2/20200903/Chloroquine25_t1_md2_graph.json\n"
     ]
    }
   ],
   "source": [
    "# Save the graph\n",
    "import networkx as nx\n",
    "import json\n",
    "# Save as gexf file\n",
    "min_mentions = graph_handle.rules['min_mentions']\n",
    "graphname = '' + category_name\n",
    "graphfileprefix = graphname + str(graph_handle.rules['max_day_old']) + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_graph'\n",
    "graphfilename = os.path.join(results_data_path, graphfileprefix +'.gexf')\n",
    "jsongraphfilename = os.path.join(results_data_path, graphfileprefix +'.json')\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)\n",
    "\n",
    "# Save as json file\n",
    "Gnld = nx.readwrite.json_graph.node_link_data(G)\n",
    "with open(jsongraphfilename, 'w') as outfile:\n",
    "    json.dump(Gnld, outfile)\n",
    "print('Wrote',jsongraphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(twitclusters)\n",
    "importlib.reload(twitutils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic processing of all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the data from the clusters\n",
    "cluster_info_dic = {}\n",
    "for c_id in clusters:\n",
    "    cgraph = clusters[c_id]\n",
    "    if cgraph.number_of_nodes()==0: #in case a cluster has been removed\n",
    "        cluster_info_dic[c_id] = {}\n",
    "        continue\n",
    "    cgraph = twitclusters.cluster_attributes(cgraph)\n",
    "    table_dic = twitclusters.cluster_tables(cgraph)\n",
    "    #node_details = \n",
    "    cluster_filename = os.path.join(results_data_path, 'cluster' + str(c_id))\n",
    "    cluster_info_dic[c_id] = {}\n",
    "    cluster_info_dic[c_id]['info_table'] = table_dic\n",
    "    #cluster_info_dic[c_id]['user_details'] = node_details\n",
    "    cluster_info_dic[c_id]['filename'] = cluster_filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding global infos\n",
    "# keywords\n",
    "corpus = twitclusters.get_corpus(cluster_info_dic)\n",
    "keyword_dic = twitclusters.tfidf(corpus)\n",
    "# save in the cluster info dic\n",
    "for c_id in clusters:\n",
    "    if clusters[c_id].number_of_nodes()>0:\n",
    "        cluster_info_dic[c_id]['info_table']['keywords'] = keyword_dic[c_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering global info\n",
    "# Saving in excel files\n",
    "for c_id in cluster_info_dic:\n",
    "    if not cluster_info_dic[c_id]:\n",
    "        continue\n",
    "    info_table = cluster_info_dic[c_id]['info_table']\n",
    "    cluster_general_info = {'cluster id': c_id, 'Nb users': clusters[c_id].number_of_nodes(), \n",
    "                           'Nb of tweets':clusters[c_id].size(weight='weight'),\n",
    "                           'Start date': str(G.graph['start_date']),\n",
    "                           'End date': str(G.graph['end_date']),\n",
    "                           'Search topic': category_name}\n",
    "                           #'cluster connectivity': c_connectivity[c_id]}\n",
    "    cluster_general_df = pd.DataFrame.from_dict([cluster_general_info])\n",
    "    #info_table = {'cluster':cluster_general_df, **info_table}\n",
    "    sheet1 = pd.concat([cluster_general_df,info_table['hashtags'],info_table['keywords']],axis=1)\n",
    "    tweet_table = info_table['text']\n",
    "    #user_table = \n",
    "    cluster_indicators = pd.DataFrame([twitclusters.compute_cluster_indicators(clusters[c_id])])\n",
    "    excel_data = {'cluster':sheet1, 'tweets':tweet_table, 'indicators': cluster_indicators, 'users': node_df}\n",
    "    #excel_data = info_table\n",
    "    twitclusters.save_excel(excel_data,cluster_info_dic[c_id]['filename'] + '_infos.xlsx', table_format='Fanny')\n",
    "    pysad.graph.save_graph(clusters[c_id],cluster_info_dic[c_id]['filename'] + 'graph.gexf')\n",
    "    tweet_table.to_csv(cluster_info_dic[c_id]['filename'] + '_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving clusters info to be displayed with the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing clusters info to the graph\n",
    "G = twitclusters.clutersprop2graph(G,cluster_info_dic,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a json file\n",
    "import networkx as nx\n",
    "\n",
    "graphname = 'graph'\n",
    "jsongraphfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_graph.json'\n",
    "\n",
    "Gnld = nx.readwrite.json_graph.node_link_data(G)\n",
    "with open(jsongraphfilename, 'w') as outfile:\n",
    "    json.dump(Gnld, outfile)\n",
    "print('Wrote',jsongraphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters info as a json file\n",
    "clusterinfotoviz = G.graph['clusters']\n",
    "jsonfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_clusters.json'\n",
    "\n",
    "with open(jsonfilename, 'w') as outfile:\n",
    "    json.dump(clusterinfotoviz, outfile)\n",
    "print('Wrote',jsonfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
